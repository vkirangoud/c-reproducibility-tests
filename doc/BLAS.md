Got it. I’ll look into whether AOCL-BLAS is bit-reproducible across different AMD hardware configurations. This includes checking if matrix operations using AOCL-BLAS produce identical binary outputs on various AMD CPUs, under the same software and build settings.

I’ll get back to you with the findings shortly.


# AOCL‑BLAS and Cross‑CPU Consistency

AMD’s AOCL‑BLAS is a tuned BLIS-based library for Zen CPUs, dynamically selecting CPU‑specific kernels at run time.  By default it chooses the “best” kernel for each processor (e.g. a Zen4-optimized GEMM on Genoa vs a Zen3 one on Milan), so identical code can produce slightly different floating‑point results on different AMD chips. In fact, documentation (and MathWorks’ guidelines) note that *bitwise* reproducibility holds only if the hardware (CPU microarchitecture, instruction set, number of threads, etc.) is unchanged. Changing the CPU family or BLAS backend typically leads to small round‑off differences.  Similarly, multithreaded execution can reorder summations or use atomic reductions, which can break bit-for-bit identity across runs.  In short, AOCL‑BLAS **does not guarantee** identical  results on EPYC vs Ryzen under normal operation.

However, AOCL‑BLAS does provide knobs to enforce a uniform code path.  For example, AMD’s AOCL documentation states that setting the environment variable `BLIS_ARCH_TYPE` (to one of `{zen4, zen3, zen2, zen, generic}`) will “completely override” the automatic dispatch.  This forces the library to use a single architecture’s kernels (for instance, specifying `zen3` on both platforms).  In practice, you can pick the lowest common denominator (e.g. `zen2` or even `generic`) so that both CPUs run the *same* code path.  Likewise, `BLIS_MODEL_TYPE={Milan, Genoa, …}` can be used to pick a particular processor model (Milan/Milan‑X=Zen3, Genoa/Bergamo=Zen4).  Using these variables in concert can yield *nearly* identical results: AMD notes that setting a fixed model/arch “may provide consistent results across different models if consistency is a higher priority than best performance”.  (Note that forcing an incompatible code path will cause errors – e.g. Zen4 kernels on a Zen3 CPU can illegal-instruct.)

**Sources of Variation:** In summary, even with the same AOCL version and data, differences arise from (1) **architecture‑specific kernels** (vector width, FMA usage, instruction latency, etc.), (2) **threading and reduction order** (atomics or OpenMP sum order), and (3) **runtime dispatch logic**.  For example, AOCL‑BLAS may use AVX2 on one CPU and AVX512 on another, or execute inner loops in a different associative order.  Multi‑threaded summations are generally not guaranteed to be bit‑for‑bit reproducible.  Indeed, MathWorks explicitly warns that identical floating‑point outputs only occur if CPU type and BLAS library are unchanged.  If strict determinism is needed, one must manually control these factors.

**Reproducibility Controls:**  To maximize consistency, use AOCL/BLIS environment options and single‑threading:

* **Force one architecture path:** Set `BLIS_ARCH_TYPE` (e.g. “zen3”) so both machines run the same kernel code. A “generic” setting uses C reference code (portable but slow) for truly uniform behavior.
* **Fix the CPU model:** Set `BLIS_MODEL_TYPE` to the same model name (Milan, Genoa, etc.) on both systems to suppress model‑specific tweaks.
* **Limit threading:** Set `BLIS_NUM_THREADS=1` (or `OMP_NUM_THREADS=1`) or use the *single‑threaded* AOCL-BLAS library. This avoids nondeterminism from parallel reduction order. Pin threads or use static scheduling if multithreaded.
* **Compiler flags:** Ensure both systems use the same floating‑point compiler modes (disable mixed precision or flush subnormals in the same way) to avoid hidden differences.
* **Library versions:** Use the identical AOCL‑BLAS binary on both systems; dynamic dispatch will only differ if the CPU differs (else no difference in code).

In practice, many users report that with these settings AOCL-BLAS produces *nearly* identical results, but perfect bit-for-bit identity is hard to guarantee outside a single CPU model. AMD provides no one-switch “reproducible mode,” but the above tips can minimize variations. If ultimate reproducibility is required, using the generic (pure-C) code path and single thread on both machines will ensure identical bitwise output – at the cost of performance.

**Practical Guidance:** In summary, AOCL-BLAS will likely yield slightly different results on different AMD CPUs unless carefully configured. To test, you can run AOCL’s own DGEMM tests or your workload with `BLIS_ARCH_DEBUG=1` to see which sub-configuration is chosen.  If differences arise, try setting `BLIS_ARCH_TYPE` (or `BLIS_MODEL_TYPE`) identically on all machines. Also compare single-threaded vs multi-threaded runs. Understanding that FP arithmetic is deterministic only for a given code path, the safest approach for strict reproducibility is to **freeze the code path and threads** as above. In most cases small differences are acceptable, but for true bit-for-bit need, use a common code path (e.g. “generic”) and a single thread.

**Sources:** AMD’s AOCL documentation describes the dynamic dispatch and environment variables for BLIS (AOCL-BLAS).  MathWorks notes that floating-point results only match exactly if CPU/threads/BLAS are unchanged. These sources (and user reports) indicate that AOCL-BLAS does *not* automatically guarantee bitwise reproducibility across different AMD chips, but it can be tuned via BLIS environment variables to achieve it.
